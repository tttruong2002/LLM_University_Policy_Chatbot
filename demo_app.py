import streamlit as st
import os
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- C·∫§U H√åNH C∆† B·∫¢N ---

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng (c·∫ßn file .env ch·ª©a GROQ_API_KEY)
try:
    load_dotenv()
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")
    if not GROQ_API_KEY:
        st.error("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y GROQ_API_KEY. Vui l√≤ng t·∫°o file .env v√† th√™m API key v√†o.")
        st.stop()
except Exception as e:
    st.error(f"‚ö†Ô∏è L·ªói khi t·∫£i file .env: {e}")
    st.stop()

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n v√† t√™n collection
DB_PATH = "./chroma_db"
COLLECTION_NAME = "academic_regulations"

# Ki·ªÉm tra xem th∆∞ m·ª•c ChromaDB c√≥ t·ªìn t·∫°i kh√¥ng
if not os.path.exists(DB_PATH):
    st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c '{DB_PATH}'.")
    st.error("Vui l√≤ng ch·∫°y file '01_Data_Ingestion.ipynb' tr∆∞·ªõc ƒë·ªÉ t·∫°o database.")
    st.stop()

# --- T·∫¢I PIPELINE RAG (S·ª¨ D·ª§NG CACHE) ---

# st.cache_resource: Ch·ªâ ch·∫°y h√†m n√†y 1 L·∫¶N DUY NH·∫§T khi app kh·ªüi ƒë·ªông
# Gi√∫p ti·∫øt ki·ªám th·ªùi gian, kh√¥ng c·∫ßn t·∫£i l·∫°i model v√† DB m·ªói khi user h·ªèi
@st.cache_resource
def load_rag_pipeline():
    """
    T·∫£i v√† kh·ªüi t·∫°o to√†n b·ªô pipeline RAG (LLM, Embedding, DB, Chain).
    """
    try:
        # 1. Kh·ªüi t·∫°o LLM (Groq)
        llm = ChatGroq(
            model="groq/compound",
            temperature=0,
            api_key=GROQ_API_KEY
        )
        
        # 2. Kh·ªüi t·∫°o Embedding Model (HuggingFace)
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # 3. T·∫£i Vector Store (Chroma)
        vectorstore = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embedding_model,
            collection_name=COLLECTION_NAME
        )
        
        # 4. T·∫°o Retriever
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # L·∫•y 5 chunks li√™n quan
        
        # 5. T·∫°o Prompt Template
        rag_template = """
        B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch, chuy√™n tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ quy ƒë·ªãnh h·ªçc thu·∫≠t c·ªßa tr∆∞·ªùng 
        ƒêH S∆∞ Ph·∫°m K·ªπ Thu·∫≠t TP.HCM d·ª±a tr√™n c√°c vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p.
        H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c, 
        ch·ªâ d·ª±a v√†o n·ªôi dung trong ph·∫ßn "VƒÉn b·∫£n tham kh·∫£o" d∆∞·ªõi ƒë√¢y.
        KH√îNG ƒë∆∞·ª£c b·ªãa ƒë·∫∑t th√¥ng tin. N·∫øu kh√¥ng t√¨m th·∫•y, h√£y n√≥i "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu."

        VƒÉn b·∫£n tham kh·∫£o:
        {context}

        C√¢u h·ªèi:
        {question}

        C√¢u tr·∫£ l·ªùi (ch·ªâ d·ª±a tr√™n vƒÉn b·∫£n):
        """
        rag_prompt = ChatPromptTemplate.from_template(rag_template)
        
        # 6. H√†m g·ªôp context
        def format_context(docs):
            return "\n\n---\n\n".join([d.page_content for d in docs])
        
        # 7. T·∫°o RAG Chain ho√†n ch·ªânh
        # (S·ª≠ d·ª•ng logic chu·∫©n c·ªßa LangChain, t∆∞∆°ng t·ª± file 02)
        rag_chain = (
            {"context": retriever | format_context, "question": RunnablePassthrough()}
            | rag_prompt
            | llm
            | StrOutputParser()
        )
        
        return rag_chain

    except Exception as e:
        # N·∫øu c√≥ l·ªói ·ªü b·∫•t k·ª≥ b∆∞·ªõc n√†o, hi·ªÉn th·ªã l·ªói v√† d·ª´ng app
        st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫£i RAG pipeline: {e}")
        st.stop()

# --- GIAO DI·ªÜN ·ª®NG D·ª§NG STREAMLIT ---

# C·∫•u h√¨nh ti√™u ƒë·ªÅ trang
st.set_page_config(page_title="Chatbot Quy ƒë·ªãnh HCMUTE", page_icon="ü§ñ")

st.title("ü§ñ Chatbot Quy ƒë·ªãnh H·ªçc v·ª• HCMUTE")
st.caption(f"Backend: Groq (compound) | DB: ChromaDB (16 files, 156 chunks) | Giao di·ªán: Streamlit")

# T·∫£i pipeline RAG
with st.spinner("‚è≥ ƒêang t·∫£i m√¥ h√¨nh LLM v√† c∆° s·ªü d·ªØ li·ªáu vector..."):
    try:
        rag_chain = load_rag_pipeline()
        st.success("‚úÖ T·∫£i th√†nh c√¥ng! Chatbot ƒë√£ s·∫µn s√†ng.")
    except Exception as e:
        # L·ªói n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b√™n trong h√†m `load_rag_pipeline`
        pass

# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat (l∆∞u trong session_state)
if "messages" not in st.session_state:
    st.session_state.messages = []

# Hi·ªÉn th·ªã c√°c tin nh·∫Øn c≈©
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# X·ª≠ l√Ω input m·ªõi t·ª´ ng∆∞·ªùi d√πng
if prompt := st.chat_input("B·∫°n mu·ªën h·ªèi g√¨ v·ªÅ quy ƒë·ªãnh c·ªßa tr∆∞·ªùng?"):
    
    # 1. Hi·ªÉn th·ªã tin nh·∫Øn c·ªßa user l√™n giao di·ªán
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. T·∫°o ph·∫£n h·ªìi t·ª´ bot
    with st.chat_message("assistant"):
        # Hi·ªáu ·ª©ng "ƒëang g√µ..."
        response_placeholder = st.empty()
        full_response = ""
        
        # B·∫Øt ƒë·∫ßu stream c√¢u tr·∫£ l·ªùi t·ª´ RAG chain
        try:
            for chunk in rag_chain.stream(prompt):
                full_response += chunk
                response_placeholder.markdown(full_response + "‚ñå") # Th√™m con tr·ªè "g√µ"
            
            # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh
            response_placeholder.markdown(full_response)
        
        except Exception as e:
            st.error(f"L·ªói khi g·ªçi RAG chain: {e}")
            full_response = "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω."
            response_placeholder.markdown(full_response)

    # 3. L∆∞u tin nh·∫Øn c·ªßa bot v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "assistant", "content": full_response})