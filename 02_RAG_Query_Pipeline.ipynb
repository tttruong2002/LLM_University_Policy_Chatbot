{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a8ca7a",
   "metadata": {},
   "source": [
    "# RAG Query Pipeline\n",
    "## 1. Imports th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1565b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ import th√†nh c√¥ng c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho RAG pipeline.\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain-\n",
    "import os\n",
    "import chromadb\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import th√†nh c√¥ng c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho RAG pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447fe5fd",
   "metadata": {},
   "source": [
    "## 2. Kh·ªüi t·∫°o M√¥ h√¨nh (LLM v√† Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74f38fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ c·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng GROQ_API_KEY t·ª´ file .env.\n",
      "‚úÖ ƒê√£ kh·ªüi t·∫°o LLM (Groq Compound).\n",
      "‚úÖ ƒê√£ kh·ªüi t·∫°o Embedding Model (multilingual-MiniLM).\n"
     ]
    }
   ],
   "source": [
    "# C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng cho Groq API Key trong file .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"‚úÖ ƒê√£ c·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng GROQ_API_KEY t·ª´ file .env.\")\n",
    "\n",
    "model = \"groq/compound\"\n",
    "# 1. Kh·ªüi t·∫°o LLM (Si√™u nhanh v√† mi·ªÖn ph√≠ t·ª´ Groq)\n",
    "# D√πng model \"groq/compound\"\n",
    "llm = ChatGroq(\n",
    "    model= model,\n",
    "    # temperature l√† tham s·ªë ƒëi·ªÅu ch·ªânh ƒë·ªô s√°ng t·∫°o c·ªßa c√¢u tr·∫£ l·ªùi\n",
    "    temperature=0 # G·∫ßn nh∆∞ = 0 ƒë·ªÉ c√¢u tr·∫£ l·ªùi b√°m s√°t context\n",
    ")\n",
    "print(f\"‚úÖ ƒê√£ kh·ªüi t·∫°o LLM (Groq Compound).\")\n",
    "\n",
    "# 2. Kh·ªüi t·∫°o Embedding Model (D√πng l·∫°i model c≈©)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o Embedding Model (multilingual-MiniLM).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51782119",
   "metadata": {},
   "source": [
    "## 3. T·∫£i Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e35958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng ChromaDB t·ª´ './chroma_db' (collection: 'academic_regulations')\n",
      "üìö S·ªë l∆∞·ª£ng chunks trong DB: 156\n"
     ]
    }
   ],
   "source": [
    "# T·∫£i l·∫°i database ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng v√† L·ªåC S·∫†CH ·ªü file 01\n",
    "db_path = \"./chroma_db\"\n",
    "collection_name = \"academic_regulations\" # ƒê·∫£m b·∫£o t√™n n√†y kh·ªõp v·ªõi file 01\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=db_path,\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "# T·∫°o m·ªôt \"retriever\" ƒë·ªÉ l·∫•y t√†i li·ªáu\n",
    "# search_kwargs={\"k\": 5}: L·∫•y 5 chunk li√™n quan nh·∫•t\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng ChromaDB t·ª´ '{db_path}' (collection: '{collection_name}')\")\n",
    "print(f\"üìö S·ªë l∆∞·ª£ng chunks trong DB: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505cc93a",
   "metadata": {},
   "source": [
    "## 4. T·∫°o Chu·ªói RAG Ho√†n ch·ªânh (End-to-End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "047d9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫°o chu·ªói RAG ho√†n ch·ªânh.\n"
     ]
    }
   ],
   "source": [
    "# 1. T·∫°o prompt RAG ch√≠nh\n",
    "# Ra l·ªánh cho LLM tr·∫£ l·ªùi D·ª∞A TR√äN context ƒë∆∞·ª£c cung c·∫•p\n",
    "rag_template = \"\"\"\n",
    "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch, chuy√™n tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ quy ƒë·ªãnh h·ªçc thu·∫≠t c·ªßa tr∆∞·ªùng \n",
    "ƒêH S∆∞ Ph·∫°m K·ªπ Thu·∫≠t TP.HCM d·ª±a tr√™n c√°c vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p.\n",
    "H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c, \n",
    "ch·ªâ d·ª±a v√†o n·ªôi dung trong ph·∫ßn \"VƒÉn b·∫£n tham kh·∫£o\" d∆∞·ªõi ƒë√¢y.\n",
    "KH√îNG ƒë∆∞·ª£c b·ªãa ƒë·∫∑t th√¥ng tin. N·∫øu kh√¥ng t√¨m th·∫•y, h√£y n√≥i \"T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu.\"\n",
    "\n",
    "VƒÉn b·∫£n tham kh·∫£o:\n",
    "{context}\n",
    "\n",
    "C√¢u h·ªèi:\n",
    "{question}\n",
    "\n",
    "C√¢u tr·∫£ l·ªùi (ch·ªâ d·ª±a tr√™n vƒÉn b·∫£n):\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "# 2. H√†m n√†y ƒë·ªÉ g·ªôp c√°c chunk t√¨m ƒë∆∞·ª£c th√†nh 1 ƒëo·∫°n text\n",
    "def format_context(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# 3. T·∫†O CHU·ªñI RAG HO√ÄN CH·ªàNH\n",
    "# ƒê√¢y l√† \"c√¥ng th·ª©c\" c·ªßa RAG:\n",
    "# 1. L·∫•y c√¢u h·ªèi g·ªëc (RunnablePassthrough)\n",
    "# 2. D√πng c√¢u ƒë√£ m·ªü r·ªông ƒë·ªÉ t√¨m context (retriever)\n",
    "# 3. G·ªôp context l·∫°i (format_context)\n",
    "# 4. Nh·ªìi context v√† c√¢u h·ªèi g·ªëc v√†o RAG prompt (rag_prompt)\n",
    "# 5. ƒê∆∞a cho LLM tr·∫£ l·ªùi (llm)\n",
    "# 6. L·∫•y k·∫øt qu·∫£ (StrOutputParser)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": RunnablePassthrough() \n",
    "                   | retriever \n",
    "                   | format_context,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ t·∫°o chu·ªói RAG ho√†n ch·ªânh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d13a7159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'list', 'data': [{'id': 'qwen/qwen3-32b', 'object': 'model', 'created': 1748396646, 'owned_by': 'Alibaba Cloud', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 40960}, {'id': 'openai/gpt-oss-20b', 'object': 'model', 'created': 1754407957, 'owned_by': 'OpenAI', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 65536}, {'id': 'llama-3.3-70b-versatile', 'object': 'model', 'created': 1733447754, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 32768}, {'id': 'openai/gpt-oss-safeguard-20b', 'object': 'model', 'created': 1761708789, 'owned_by': 'OpenAI', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 65536}, {'id': 'moonshotai/kimi-k2-instruct', 'object': 'model', 'created': 1752435491, 'owned_by': 'Moonshot AI', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 16384}, {'id': 'openai/gpt-oss-120b', 'object': 'model', 'created': 1754408224, 'owned_by': 'OpenAI', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 65536}, {'id': 'meta-llama/llama-prompt-guard-2-22m', 'object': 'model', 'created': 1748632101, 'owned_by': 'Meta', 'active': True, 'context_window': 512, 'public_apps': None, 'max_completion_tokens': 512}, {'id': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'object': 'model', 'created': 1743877158, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'moonshotai/kimi-k2-instruct-0905', 'object': 'model', 'created': 1757046093, 'owned_by': 'Moonshot AI', 'active': True, 'context_window': 262144, 'public_apps': None, 'max_completion_tokens': 16384}, {'id': 'playai-tts-arabic', 'object': 'model', 'created': 1740682783, 'owned_by': 'PlayAI', 'active': True, 'context_window': 8192, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'groq/compound', 'object': 'model', 'created': 1756949530, 'owned_by': 'Groq', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'meta-llama/llama-4-scout-17b-16e-instruct', 'object': 'model', 'created': 1743874824, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'meta-llama/llama-prompt-guard-2-86m', 'object': 'model', 'created': 1748632165, 'owned_by': 'Meta', 'active': True, 'context_window': 512, 'public_apps': None, 'max_completion_tokens': 512}, {'id': 'allam-2-7b', 'object': 'model', 'created': 1737672203, 'owned_by': 'SDAIA', 'active': True, 'context_window': 4096, 'public_apps': None, 'max_completion_tokens': 4096}, {'id': 'playai-tts', 'object': 'model', 'created': 1740682771, 'owned_by': 'PlayAI', 'active': True, 'context_window': 8192, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'llama-3.1-8b-instant', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 131072}, {'id': 'whisper-large-v3', 'object': 'model', 'created': 1693721698, 'owned_by': 'OpenAI', 'active': True, 'context_window': 448, 'public_apps': None, 'max_completion_tokens': 448}, {'id': 'groq/compound-mini', 'object': 'model', 'created': 1756949707, 'owned_by': 'Groq', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'meta-llama/llama-guard-4-12b', 'object': 'model', 'created': 1746743847, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 1024}, {'id': 'whisper-large-v3-turbo', 'object': 'model', 'created': 1728413088, 'owned_by': 'OpenAI', 'active': True, 'context_window': 448, 'public_apps': None, 'max_completion_tokens': 448}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe22e63",
   "metadata": {},
   "source": [
    "## 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a5c13c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒêang th·ª±c hi·ªán truy v·∫•n RAG cho c√¢u h·ªèi: 'th·ªùi gian ƒëkmh l√† khi n√†o?'\n",
      "\n",
      "================================================================================\n",
      "**Th·ªùi gian ƒëƒÉng k√Ω m√¥n h·ªçc (ƒëkmh) trong h·ªçc k·ª≥:**\n",
      "\n",
      "- **ƒê·ª£t‚ÄØ1:** t·ª´ ng√†y **11‚ÄØ/‚ÄØ08‚ÄØ/‚ÄØ2025** ƒë·∫øn h·∫øt ng√†y **23‚ÄØ/‚ÄØ08‚ÄØ/‚ÄØ2025**.  \n",
      "- **ƒê·ª£t‚ÄØ2:** t·ª´ ng√†y **27‚ÄØ/‚ÄØ10‚ÄØ/‚ÄØ2025** ƒë·∫øn h·∫øt ng√†y **01‚ÄØ/‚ÄØ11‚ÄØ/‚ÄØ2025**.\n",
      "================================================================================\n",
      "\n",
      "üìÑ C√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß:\n",
      "**Th·ªùi gian ƒëƒÉng k√Ω m√¥n h·ªçc (ƒëkmh) trong h·ªçc k·ª≥:**\n",
      "\n",
      "- **ƒê·ª£t‚ÄØ1:** t·ª´ ng√†y **11‚ÄØ/‚ÄØ08‚ÄØ/‚ÄØ2025** ƒë·∫øn h·∫øt ng√†y **23‚ÄØ/‚ÄØ08‚ÄØ/‚ÄØ2025**.  \n",
      "- **ƒê·ª£t‚ÄØ2:** t·ª´ ng√†y **27‚ÄØ/‚ÄØ10‚ÄØ/‚ÄØ2025** ƒë·∫øn h·∫øt ng√†y **01‚ÄØ/‚ÄØ11‚ÄØ/‚ÄØ2025**.\n",
      "\n",
      "‚úÖ Ho√†n t·∫•t!\n"
     ]
    }
   ],
   "source": [
    "# ƒê·∫∑t c√¢u h·ªèi (vi·∫øt t·∫Øt)\n",
    "user_query = \"th·ªùi gian ƒëkmh l√† khi n√†o?\"\n",
    "\n",
    "print(f\"üîç ƒêang th·ª±c hi·ªán truy v·∫•n RAG cho c√¢u h·ªèi: '{user_query}'\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# D√πng .stream() ƒë·ªÉ xem LLM \"g√µ\" c√¢u tr·∫£ l·ªùi\n",
    "full_response = \"\"\n",
    "\n",
    "for chunk in rag_chain.stream(user_query):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    full_response += chunk\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\nüìÑ C√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß:\\n{full_response}\")\n",
    "print(\"\\n‚úÖ Ho√†n t·∫•t!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
